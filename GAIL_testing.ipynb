{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing GAIL on a Safety GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the environment to test on.\n",
    "\n",
    "Using a [grid world with lava](https://github.com/maximecb/gym-minigrid#distributional-shift-environment), based on the DeepMind RL Safety Envs, to test generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T01:20:15.512319Z",
     "start_time": "2020-05-01T01:20:15.497422Z"
    },
    "code_folding": [
     33
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FullyObsWrapper, FlatObsWrapper\n",
    "from gym_minigrid.minigrid import WorldObj, IDX_TO_OBJECT, DIR_TO_VEC\n",
    "\n",
    "from stable_baselines import GAIL, PPO2\n",
    "from stable_baselines.gail import ExpertDataset, generate_expert_traj\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "from rl_baselines_zoo.utils import make_env\n",
    "\n",
    "from util.stable_baseline_viz import show_videos, record_video\n",
    "import numpy as np\n",
    "\n",
    "# the main environment we're testing on\n",
    "ENV_ID = 'MiniGrid-DistShift1-v0'\n",
    "\n",
    "# use the slightly different env for later testing of generalization\n",
    "MODIFIED_ENV_ID = 'MiniGrid-DistShift2-v0'\n",
    "\n",
    "# file I/O configuration\n",
    "SIM_HOME = 'gail_data'\n",
    "\n",
    "# training logging\n",
    "LOG_DIR_BASE = './logs/'\n",
    "\n",
    "# these are for data associated with the expert\n",
    "EXPERT_NAME = 'ppo2'\n",
    "EXPERT_LOG_DIR = os.path.join(LOG_DIR_BASE, EXPERT_NAME)\n",
    "EXPERT_DIR = os.path.join(SIM_HOME, 'expert_data')\n",
    "EXPERT_RUN_ID = f'expert_{EXPERT_NAME}_{ENV_ID}'\n",
    "EXPERT_MODEL_PATH = os.path.join(EXPERT_DIR, f'{EXPERT_RUN_ID}_model')\n",
    "EXPERT_MODEL_TRACES_PATH = os.path.join(EXPERT_DIR, f'{EXPERT_RUN_ID}.npz')\n",
    "\n",
    "# these are for data associated with the imitation learner\n",
    "LEARNER_NAME = 'GAIL'\n",
    "LEARNER_DIR = os.path.join(SIM_HOME, 'learner_data')\n",
    "LEARNER_LOG_DIR = os.path.join(LOG_DIR_BASE, LEARNER_NAME)\n",
    "LEARNER_RUN_ID = f'learner_{LEARNER_NAME}_{ENV_ID}'\n",
    "LEARNER_MODEL_PATH = os.path.join(LEARNER_DIR, f'{LEARNER_RUN_ID}_model')\n",
    "\n",
    "# need to ensure these directories always exist\n",
    "Path(EXPERT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(LEARNER_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOG_DIR_BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# decide whether you want to load in a pre-trained expert for the ENV or if\n",
    "# you need to learn an expert using RL data\n",
    "expert_formats = ['pre_trained_model', 'traces_only', 'learn_the_model']\n",
    "expert_format = 'traces_only'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Underlying MDP definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:40:14.950034Z",
     "start_time": "2020-05-01T00:40:14.888621Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MDP_DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# this function is essentially defining the MDP's state space\n",
    "class FlatFullyObsStateWrapper(gym.core.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Returns the agent's position and direction as 1-D vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent_env):\n",
    "        super().__init__(parent_env)\n",
    "        \n",
    "        self.parent_env = parent_env\n",
    "        \n",
    "        # the width / height include 1 cell padding on either side, so\n",
    "        # an agent coordinate will be from 1 -- [(width/height) - 2]\n",
    "        agent_max_x = parent_env.width - 2\n",
    "        agent_max_y = parent_env.height - 2\n",
    "        possible_state_label_IDs = list(IDX_TO_OBJECT.keys())\n",
    "        possible_agent_direction_vectors = DIR_TO_VEC\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            # direction part of state is from 0-4, so lowest observed value\n",
    "            # in state vector is 0\n",
    "            low=0,\n",
    "            # we are keeping x and y as separate coordinates of the state\n",
    "            # so the max value is the max of all types of vars in the state\n",
    "            high=np.max([agent_max_x, agent_max_y,\n",
    "                         np.max(possible_state_label_IDs),\n",
    "                         len(possible_agent_direction_vectors)]),\n",
    "            # we're going to store the state as:\n",
    "            #     [pos_x, pos_y, direction, state_label_ID]\n",
    "            shape=(4,),\n",
    "            # sigh, this needs to be float or GAIl stuff will freak out\n",
    "            dtype='float32'\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        agent_x = self.parent_env.agent_pos[0]\n",
    "        agent_y = self.parent_env.agent_pos[1]\n",
    "        \n",
    "        # here we're going to extract the MDP state label for the current \n",
    "        # grid location to help the learner generalize\n",
    "        # \n",
    "        # IDX_TO_OBJECT[state_label_ID] is something like \"lava\" or \"goal\"\n",
    "        grid = self.parent_env.grid.encode()\n",
    "        state_label_ID, _, _ = grid[agent_x, agent_y]\n",
    "\n",
    "        obs = np.array([agent_x,\n",
    "                        agent_y,\n",
    "                        self.parent_env.agent_dir,\n",
    "                        state_label_ID], dtype=np.float32)\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Model Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:57:20.865829Z",
     "start_time": "2020-05-01T00:57:18.226Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = {'expert_ppo2':\n",
    "                    {'cliprange': 0.2,\n",
    "                     'ent_coef': 0.0,\n",
    "                     'gamma': MDP_DISCOUNT_FACTOR,\n",
    "                     'lam': 0.95,\n",
    "                     'learning_rate': 0.00025,\n",
    "                     'n_steps': 128,\n",
    "                     'n_timesteps': 70_000,\n",
    "                     'nminibatches': 32,\n",
    "                     'noptepochs': 10,\n",
    "                     'policy': 'MlpPolicy'},\n",
    "               'gail':\n",
    "                    {'gamma': MDP_DISCOUNT_FACTOR\n",
    "                    }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:57:25.155911Z",
     "start_time": "2020-05-01T00:57:25.152964Z"
    }
   },
   "outputs": [],
   "source": [
    "env_wrapper = FlatFullyObsStateWrapper\n",
    "normalize = False\n",
    "n_envs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Definition w.r.t. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:57:26.585281Z",
     "start_time": "2020-05-01T00:57:26.554801Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferg/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Here, we need to make it fully observable and then compatible with \n",
    "# stable-baselines\n",
    "env = DummyVecEnv([make_env(ENV_ID, wrapper_class=env_wrapper,\n",
    "                             rank=i) for i in range(n_envs)])\n",
    "if normalize:\n",
    "    env = VecNormalize(env)\n",
    "    \n",
    "# need a copy of this environment for online algorithm evaluation\n",
    "eval_env = DummyVecEnv([make_env(ENV_ID, wrapper_class=env_wrapper,\n",
    "                                 rank=i) for i in range(1)])\n",
    "if normalize:\n",
    "    eval_env = VecNormalize(eval_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an expert demonstrator and generate expert trajectories\n",
    "\n",
    "The end goal of this section is to provide the imitation learner with a set of \"expert\" demonstrations to learn from. This can be accomplished in several ways:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Expert Formats:**\n",
    "\n",
    "*possible formats: `'pre_trained_model', 'traces_only', 'learn_the_model'`*\n",
    "\n",
    "* `'learn_the_model'`: learning the model just requires choosing the desired RL algorithm and settings its hyperparameters (see above).\n",
    "\n",
    "* `'pre_trained_model'`: a pre-trained expert must have a saved `stable_baselines` model file at `EXPERT_MODEL_PATH`. See the [saving guide](https://stable-baselines.readthedocs.io/en/master/guide/save_format.html) for info on how to do that.\n",
    "\n",
    "* `'traces_only'`: demonstration traces must reside in the `npz` archive at `EXPERT_MODEL_TRACES_PATH` and folow the format needed by `stable_baselines.gail.ExpertDataset()`.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "--- \n",
    "\n",
    "<br>\n",
    "\n",
    "**From the docs**:\n",
    "\n",
    "*The expert dataset is a .npz archive. The data is saved in python dictionary format with keys: actions, episode_returns, rewards, obs, episode_starts.*\n",
    "\n",
    "*In case of images, obs contains the relative path to the images.*\n",
    "\n",
    "*obs, actions: shape (N * L, ) + S*\n",
    "\n",
    "*where N = # episodes, L = episode length and S is the environment observation/action space.*\n",
    "\n",
    "*S = (1, ) for discrete space*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T01:33:35.643727Z",
     "start_time": "2020-05-01T01:33:35.636347Z"
    }
   },
   "outputs": [],
   "source": [
    "if expert_format == 'pre_trained_model':\n",
    "    expert_model = PPO2.load(EXPERT_MODEL_PATH)\n",
    "    expert_model.set_env(env)\n",
    "\n",
    "elif expert_format == 'learn_the_model':\n",
    "    expert_hparam = hyperparams['expert_ppo2']\n",
    "\n",
    "    expert_model = PPO2(expert_hparam['policy'], env,\n",
    "                        cliprange=expert_hparam['cliprange'],\n",
    "                        ent_coef=expert_hparam['ent_coef'],\n",
    "                        gamma=expert_hparam['gamma'],\n",
    "                        lam=expert_hparam['lam'],\n",
    "                        learning_rate=expert_hparam['learning_rate'],\n",
    "                        n_steps=expert_hparam['n_steps'],\n",
    "                        nminibatches=expert_hparam['nminibatches'],\n",
    "                        noptepochs=expert_hparam['noptepochs'],\n",
    "                        verbose=0,\n",
    "                        tensorboard_log=EXPERT_LOG_DIR)\n",
    "\n",
    "    # always use deterministic actions for live evaluation\n",
    "    eval_callback = EvalCallback(eval_env,\n",
    "                                 best_model_save_path=EXPERT_LOG_DIR,\n",
    "                                 log_path=EXPERT_LOG_DIR, eval_freq=500,\n",
    "                                 deterministic=True, render=False)\n",
    "\n",
    "    # while evaluate the model on a new environment and save the best one\n",
    "    # periodically\n",
    "    expert_model.learn(total_timesteps=expert_hparam['n_timesteps'],\n",
    "                       callback=eval_callback)\n",
    "    \n",
    "    # now save the final model so if we like it, we don't need to re-learn it\n",
    "    # The model will be saved under $EXPERT_MODEL_EXPER_MODEL_PATH.zip\n",
    "    expert_model.save(EXPERT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now have an expert model, sample trajectories from it and save them as demonstrations for the imitation learner. If you just provide traces, we will do nothing and assume the traces exist at `EXPERT_MODEL_TRACES_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T01:38:28.045732Z",
     "start_time": "2020-05-01T01:38:28.007900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (1417, 1)\n",
      "obs (1417, 4)\n",
      "rewards (1417, 1)\n",
      "episode_returns (100,)\n",
      "episode_starts (1417,)\n",
      "Total trajectories: -1\n",
      "Total transitions: 1417\n",
      "Average returns: 0.9395714378356934\n",
      "Std for returns: 0.09469160588637068\n"
     ]
    }
   ],
   "source": [
    "if expert_format != 'traces_only':\n",
    "    \n",
    "    # vectorized environments do not work with generate_expert_traj, not sure\n",
    "    # why. Seems to be that threads are probably not synced properly somehow\n",
    "    #\n",
    "    # The evaluation environment already must only have a singular env, so\n",
    "    # use it for trace generation.\n",
    "    expert_model.set_env(eval_env)\n",
    "    \n",
    "    # generate trajectories in the environment under the expert_model\n",
    "    generate_expert_traj(expert_model, EXPERT_MODEL_TRACES_PATH,\n",
    "                         n_episodes=100);\n",
    "    \n",
    "# Load the expert dataset\n",
    "dataset = ExpertDataset(expert_path=EXPERT_MODEL_TRACES_PATH, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the imitation learner from the expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T01:37:55.367557Z",
     "start_time": "2020-05-01T01:37:55.351472Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expert_model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-422c3cd57139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the expert dataset and define the GAIL model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpert_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m gail_model = GAIL('MlpPolicy', env, dataset, verbose=1,\n\u001b[1;32m      6\u001b[0m                   tensorboard_log='logs')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expert_model_path' is not defined"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "gail_model = GAIL('MlpPolicy', env, dataset, verbose=1,\n",
    "                  tensorboard_log=LEARNER_LOG_DIR)\n",
    "\n",
    "# Note: in practice, you need to train for 1M steps to have a working policy\n",
    "gail_model.learn(total_timesteps=100_000)\n",
    "gail_model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Learned Expert on the Original Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:40:15.936758Z",
     "start_time": "2020-05-01T00:40:14.739Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "record_video(expert_model, eval_env=vec_env, max_video_length=500,\n",
    "             video_prefix=f'ppo2_expert_{ENV_ID}')\n",
    "show_videos('videos', prefix='ppo2_expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the learned expert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:40:15.937363Z",
     "start_time": "2020-05-01T00:40:14.896Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(expert_model, vec_env,\n",
    "                                          n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Imitation Learner on the Orginal Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:40:15.937941Z",
     "start_time": "2020-05-01T00:40:15.045Z"
    }
   },
   "outputs": [],
   "source": [
    "record_video(gail_model, eval_env=vec_env, max_video_length=500,\n",
    "             video_prefix=f'gail_{ENV_ID}')\n",
    "show_videos('videos', prefix='gail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the learned expert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T00:40:15.938500Z",
     "start_time": "2020-05-01T00:40:15.196Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(gail_model, vec_env,\n",
    "                                          n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
